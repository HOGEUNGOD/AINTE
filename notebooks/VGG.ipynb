{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import savgol_filter\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "\n",
    "def data_load(path):\n",
    "    files = glob.glob(path)\n",
    "    if not files:\n",
    "        print(\"Chck Phath\")\n",
    "        sys.exit()\n",
    "\n",
    "    _feature=[]\n",
    "    label=[]\n",
    "    for address in tqdm.tqdm(files):\n",
    "        label_ = os.path.basename(address)\n",
    "        for num in glob.glob(address+'/*'):\n",
    "            _feature.append(np.load(num))\n",
    "            label.append([label_])\n",
    "        print(label_)\n",
    "    target = np.array(label).flatten()\n",
    "    return _feature, target.reshape((-1,))\n",
    "\n",
    "def data_split(feature, target):\n",
    "    a_set, b_set, a_target, b_target = [], [], [], []\n",
    "    arr = np.arange(80)\n",
    "    np.random.seed(18)\n",
    "    np.random.shuffle(arr)\n",
    "    arr = np.split(arr,2)\n",
    "    for batch in range(10):\n",
    "        batch_feature_array = feature[0+80*batch:80+80*batch]\n",
    "        batch_target_array = target[0+80*batch:80+80*batch]\n",
    "        for count in arr[0]:\n",
    "            a_set.append(batch_feature_array[count])\n",
    "            a_target.append(batch_target_array[count])\n",
    "        for count in arr[1]:\n",
    "            b_set.append(batch_feature_array[count])\n",
    "            b_target.append(batch_target_array[count])\n",
    "    return np.array(a_set), np.array(b_set), np.array(a_target), np.array(b_target)\n",
    "\n",
    "\n",
    "X_train, y_train = data_load('../train_data/*')\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "X_test, y_test = data_load('../test_data/*')\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "X_test,  X_valid, y_test, y_valid = data_split(X_test, y_test)\n",
    "X_train = np.array(X_train) / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_test_raw = y_test.copy()\n",
    "y_test = to_categorical(y_test)\n",
    "number_of_classes = y_valid.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensorflow.random.set_seed(7)\n",
    "\n",
    "Conv_default = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3,padding=\"SAME\")\n",
    "model = keras.models.Sequential([\n",
    "    Conv_default(filters=64, kernel_size=3, input_shape=[210, 210, 10]),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    Conv_default(filters=64),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv_default(filters=128),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    Conv_default(filters=128),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv_default(filters=256),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    Conv_default(filters=256),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv_default(filters=512),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    Conv_default(filters=512),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv_default(filters=512),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    Conv_default(filters=512),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(),\n",
    "              metrics=[\"mse\", 'mse'])\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=300, callbacks=[callback], validation_data=(X_valid, y_valid))\n",
    "print('Time :', time.time()-start)\n",
    "score = model.evaluate(X_test,y_test)\n",
    "print(score)\n",
    "\n",
    "\n",
    "model.save(\"VGG2.h5\")\n",
    "np.save('history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "history=np.load('history.npy',allow_pickle='TRUE').item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "training_loss = history[\"loss\"]\n",
    "test_loss = history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss)+1)\n",
    "\n",
    "training_acc = history[\"accuracy\"]\n",
    "test_acc = history[\"val_accuracy\"]\n",
    "\n",
    "ax.plot(epoch_count, training_loss, \"r-\")\n",
    "ax.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"training_loss\", \"validation_loss\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.savefig('./figure/loss.png',dpi=300,bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax.plot(epoch_count, training_acc, \"r-\")\n",
    "ax.plot(epoch_count, test_acc, \"b-\")\n",
    "ax.legend([\"training_acc\", \"validation_acc\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim(0,1)\n",
    "plt.savefig('./figure/acc.png',dpi=300,bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}